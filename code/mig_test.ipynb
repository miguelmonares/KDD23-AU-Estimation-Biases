{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./code')\n",
    "import argparse\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import argparse\n",
    "from vgg_face import *\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
    "plt.switch_backend('agg')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False if feature_extracting else True\n",
    "\n",
    "def centercrop_opencv(image, size):\n",
    "    height, width, _ = image.shape\n",
    "    crop_height = crop_width = size\n",
    "    start_height = (height - crop_height) // 2\n",
    "    start_width = (width - crop_width) // 2\n",
    "    cropped_image = image[start_height:start_height+crop_height, start_width:start_width+crop_width]\n",
    "    return cropped_image\n",
    "\n",
    "def test_model(model, crop_frame, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        crop_frame = cv2.cvtColor(crop_frame, cv2.COLOR_BGR2RGB)\n",
    "        crop_frame = cv2.resize(crop_frame, (256, 256))\n",
    "        crop_frame = centercrop_opencv(crop_frame, 224)\n",
    "        crop_frame = crop_frame / 255.0\n",
    "        crop_frame = np.transpose(crop_frame, (2, 0, 1))\n",
    "        crop_frame = np.expand_dims(crop_frame, axis=0)  # add batch dimension\n",
    "        crop_frame = torch.from_numpy(crop_frame).float().to(device)\n",
    "        output = model(crop_frame)\n",
    "        output = output * torch.FloatTensor([16] + [5]*9 + [25]).to(device)\n",
    "    return output\n",
    "\n",
    "def plot_bar(au_scores, title):\n",
    "    #labels = ['AU4', 'AU6', 'AU7', 'AU9', 'AU10', 'AU12', 'AU20', 'AU25', 'AU26', 'AU43']\n",
    "    labels = ['Brow\\nLowerer', 'Cheek\\nRaiser', 'Lid\\nTightener', 'Nose\\nWrinkler', 'Upper Lip\\nRaiser', 'Lip Corner\\nPuller', 'Lip\\nStretcher', 'Lips\\nPart', 'Jaw\\nDrop', 'Eyes\\nClosed']\n",
    "    colors = ['blue', 'orange', 'green', 'red', 'purple', 'brown', 'pink', 'gray', 'olive', 'burlywood']\n",
    "    fig, ax = plt.subplots(figsize=(10, 10.8))\n",
    "    values = au_scores[1:]\n",
    "    fig, ax = plt.subplots(figsize=(10, 10.8))\n",
    "\n",
    "    #ax.barh(labels, values, color=colors)\n",
    "    ordertoshow=[7, 6, 5, 4, 3, 2, 8, 0, 1, 9]\n",
    "    ax.barh(np.take(labels, ordertoshow), np.take(values, ordertoshow), color=np.take(colors, ordertoshow))\n",
    "\n",
    "    ax.tick_params(axis='both', labelsize=15)\n",
    "    ax.set_title(title + ' PSPI {:.2f}/16'.format(au_scores[0]), fontsize=20)\n",
    "    ax.set_xlim([0, 5])\n",
    "    canvas = FigureCanvasAgg(fig)\n",
    "    canvas.draw()\n",
    "    plot = np.array(canvas.renderer.buffer_rgba())\n",
    "    plot = cv2.cvtColor(plot, cv2.COLOR_RGB2BGR)\n",
    "    plt.close()\n",
    "    return plot\n",
    "\n",
    "def rounded_rectangle(image, x, y, w, h, radius, thickness):\n",
    "    cv2.ellipse(image, (x + radius, y + radius), (radius, radius), 180, 0, 90, (51,255,255), thickness)\n",
    "    cv2.ellipse(image, (x + w - radius, y + radius), (radius, radius), 270, 0, 90, (51,255,255), thickness)\n",
    "    cv2.ellipse(image, (x + radius, y + h - radius), (radius, radius), 90, 0, 90, (51,255,255), thickness)\n",
    "    cv2.ellipse(image, (x + w - radius, y + h - radius), (radius, radius), 0, 0, 90, (51,255,255), thickness)\n",
    "    return image\n",
    "\n",
    "def adaptive_canvas(image_left, image_right):\n",
    "    h_l, w_l, _ = image_left.shape\n",
    "    h_r, w_r, _ = image_right.shape\n",
    "    canvas_h = max(h_l, h_r)\n",
    "    canvas_w = w_l+w_r\n",
    "    canvas = np.zeros((canvas_h, canvas_w, 3), dtype=np.uint8)\n",
    "    canvas[:h_l, :w_l, :] = image_left[:,:,:3]\n",
    "    canvas[:h_r, w_l:, :] = image_right[:,:,:3]\n",
    "    return canvas"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "num_classes = 11\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "model = VGG_16().to(device)\n",
    "set_parameter_requires_grad(model, feature_extracting=False)\n",
    "num_ftrs = model.fc8.in_features\n",
    "model.fc8 = nn.Linear(num_ftrs, num_classes).to(device)\n",
    "model.load_state_dict(torch.load('/Users/miguelmonares/Downloads/drive-download-20230524T190919Z-001/checkpoint_epoch69_11.pth', map_location=device))\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_alt_tree.xml')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class args:\n",
    "    scale = 80\n",
    "    round = 1\n",
    "    square = 0\n",
    "    sampling_rate = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'crop_frame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 13\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m args\u001B[38;5;241m.\u001B[39msquare \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m     11\u001B[0m         frame_flip \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mrectangle(frame_flip, (x\u001B[38;5;241m-\u001B[39margs\u001B[38;5;241m.\u001B[39mscale, y\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m\u001B[38;5;241m*\u001B[39margs\u001B[38;5;241m.\u001B[39mscale), (x\u001B[38;5;241m+\u001B[39mw\u001B[38;5;241m+\u001B[39margs\u001B[38;5;241m.\u001B[39mscale, y\u001B[38;5;241m+\u001B[39mh\u001B[38;5;241m+\u001B[39margs\u001B[38;5;241m.\u001B[39mscale), (\u001B[38;5;241m255\u001B[39m, \u001B[38;5;241m255\u001B[39m, \u001B[38;5;241m255\u001B[39m), \u001B[38;5;241m4\u001B[39m)\n\u001B[0;32m---> 13\u001B[0m cv2\u001B[38;5;241m.\u001B[39mimshow(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimg\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[43mcrop_frame\u001B[49m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'crop_frame' is not defined"
     ]
    }
   ],
   "source": [
    "frame = cv2.imread('/Users/miguelmonares/FAU/images/European_Man/em1/4em1_4.10.png')\n",
    "\n",
    "frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "faces = face_cascade.detectMultiScale(frame_gray, 1.1, 6)\n",
    "for (x, y, w, h) in faces:\n",
    "    h = w\n",
    "    crop_frame = np.copy(frame_flip)[y-2*args.scale:y+h+args.scale, x-args.scale:x+w+args.scale]\n",
    "    if args.round == 1:\n",
    "        frame_flip = rounded_rectangle(frame_flip, x, y, w, h, 100, 20)\n",
    "    if args.square == 1:\n",
    "        frame_flip = cv2.rectangle(frame_flip, (x-args.scale, y-2*args.scale), (x+w+args.scale, y+h+args.scale), (255, 255, 255), 4)\n",
    "\n",
    "cv2.imshow('img',crop_frame)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'crop_frame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m cv2\u001B[38;5;241m.\u001B[39mimshow(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimg\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[43mcrop_frame\u001B[49m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'crop_frame' is not defined"
     ]
    }
   ],
   "source": [
    "cv2.imshow('img',crop_frame)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# set up a camera window\n",
    "cv2.namedWindow('face_cap', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('face_cap', 1500, 600)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*\"MJPG\"))\n",
    "cap_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "cap_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "cap_info = '({}*{}, fps {})'.format(cap_w, cap_h, fps)\n",
    "\n",
    "save_count=0\n",
    "while(True):\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # mirror the video\n",
    "    frame_flip = cv2.flip(frame,1)\n",
    "\n",
    "    # opencv2 detect face\n",
    "    frame_gray = cv2.cvtColor(frame_flip, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(frame_gray, 1.1, 6)\n",
    "    for (x, y, w, h) in faces:\n",
    "        print(\"A\")\n",
    "        h = w\n",
    "        crop_frame = np.copy(frame_flip)[y-2*args.scale:y+h+args.scale, x-args.scale:x+w+args.scale]\n",
    "        if args.round == 1:\n",
    "            frame_flip = rounded_rectangle(frame_flip, x, y, w, h, 100, 20)\n",
    "        if args.square == 1:\n",
    "            frame_flip = cv2.rectangle(frame_flip, (x-args.scale, y-2*args.scale), (x+w+args.scale, y+h+args.scale), (255, 255, 255), 4)\n",
    "\n",
    "    # feed the frame into the model\n",
    "    try:\n",
    "        output = test_model(model, crop_frame, device)\n",
    "        output = output.flatten()\n",
    "        output_cpu = torch.Tensor.cpu(output).numpy()\n",
    "        output_plot = plot_bar(output_cpu, cap_info)\n",
    "        output_text = '|PSPI {:.2f} |au4 {:.2f} |au6 {:.2f} |au7 {:.2f} |au9 {:.2f} |au10 {:.2f} |au12 {:.2f} |au20 {:.2f} |au25 {:.2f} |au26 {:.2f} |au43 {:.2f}|'.format(output[0].item(), output[1].item(), output[2].item(), output[3].item(), output[4].item(), output[5].item(), output[6].item(), output[7].item(), output[8].item(), output[9].item(), output[10].item())\n",
    "        print(output_text)\n",
    "\n",
    "        # save video option\n",
    "\n",
    "\n",
    "    except Exception as error:\n",
    "        output_plot = canvas = np.ones((1080, 1000, 3), dtype=np.uint8)*255\n",
    "        output_text = str(error)\n",
    "\n",
    "    # print AU info at the bottom of each frame\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.8\n",
    "    #thickness = 2\n",
    "    text_size, _ = cv2.getTextSize(output_text, font, font_scale, 2)\n",
    "    text_x = int((cap_w - text_size[0]) / 2)\n",
    "    text_y = cap_h - text_size[1] - 10\n",
    "    cv2.putText(frame_flip, output_text, (text_x, text_y), font, font_scale, (255, 255, 255), 2)\n",
    "\n",
    "    # dispay canvas\n",
    "    canvas = adaptive_canvas(frame_flip, output_plot)\n",
    "    plt.close()\n",
    "    cv2.imshow('face_cap', canvas)\n",
    "\n",
    "    # Press 'Q' to quit camera\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "    # After the loop release the cap object\n",
    "cap.release()\n",
    "# Destroy all the windows\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/7ew9_7.10.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/7ew9_7.4.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/6ew9_6.8.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/9ew9_9.6.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/25ew9_25.4.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/25ew9_25.6.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/9ew9_9.4.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/10ew9_10.2.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/7ew9_7.6.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/26ew9_26.2.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/12ew9_12.8.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/26ew9_26.6.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/6ew9_6.10.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/7ew9_7.2.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/10ew9_10.6.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/25ew9_25.2.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/9ew9_9.2.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/10ew9_10.4.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/26ew9_26.4.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/20ew9_20.6.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/20ew9_20.4.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/12ew9_12.10.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/4ew9_4.8.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/20ew9_20.2.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/10ew9_10.10.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/4ew9_4.2.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/26ew9_26.10.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/20ew9_20.8.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/4ew9_4.4.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/4ew9_4.6.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/ew9.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/10ew9_10.8.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/26ew9_26.8.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/12ew9_12.2.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/9ew9_9.10.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/4ew9_4.10.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/6ew9_6.2.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/9ew9_9.8.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/6ew9_6.6.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/20ew9_20.10.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/12ew9_12.4.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/43ew9_43.10.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/12ew9_12.6.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/7ew9_7.8.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/25ew9_25.10.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/6ew9_6.4.png\n",
      "Cropped image saved: /Users/miguelmonares/FAU2/images/European_Woman/ew9/25ew9_25.8.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "def crop_images(input_folder, output_folder, x, y, width, height):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Get a list of all files in the input folder\n",
    "    file_list = os.listdir(input_folder)\n",
    "\n",
    "    # Loop through each file in the input folder\n",
    "    for file_name in file_list:\n",
    "        # Check if the file is an image\n",
    "        if file_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            # Read the image\n",
    "            image_path = os.path.join(input_folder, file_name)\n",
    "            image = cv2.imread(image_path)\n",
    "\n",
    "            # Crop the image\n",
    "            cropped_image = image[y:y+height, x:x+width]\n",
    "\n",
    "            # Save the cropped image\n",
    "            output_path = os.path.join(output_folder, file_name)\n",
    "            cv2.imwrite(output_path, cropped_image)\n",
    "\n",
    "            print(f\"Cropped image saved: {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_folder = \"/Users/miguelmonares/FAU/images/European_Woman/ew9\"\n",
    "output_folder = \"/Users/miguelmonares/FAU2/images/European_Woman/ew9\"\n",
    "x = 880  # x-coordinate of the top-left corner of the crop rectangle\n",
    "y = 180  # y-coordinate of the top-left corner of the crop rectangle\n",
    "width = 750  # Width of the crop rectangle\n",
    "height = 820  # Height of the crop rectangle\n",
    "\n",
    "crop_images(input_folder, output_folder, x, y, width, height)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}